{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7rX7Hu9Co_Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "KWW4D9Hyo2RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "data_dir = \"/content/drive/MyDrive/chest_xray/chest_xray\"\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Step 2: Model Architecture\n",
        "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(img_size[0], img_size[1], 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Step 3: Model Compilation\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Model Training\n",
        "epochs = 20\n",
        "history = model.fit(train_generator, validation_data=validation_generator, epochs=epochs)\n",
        "\n",
        "# Step 5: Model Evaluation\n",
        "test_generator = train_datagen.flow_from_directory(data_dir, target_size=img_size, batch_size=1, subset='validation', shuffle=False)\n",
        "y_true = test_generator.classes\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Step 6: Performance Evaluation\n",
        "class_names = [\"normal\", \"pneumonia\"]\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Step 7: Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LZZluY7xmfNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}